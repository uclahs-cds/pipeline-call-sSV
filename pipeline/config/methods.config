import nextflow.util.SysHelper

manifest {
    name = "call-sSV"
    mainScript = "call-sSV.nf"
    nextflowVersion = ">=20.07.1"
    author = "Yu Pan, Ghouse Mohammed"
    homePage = "https://github.com/uclahs-cds/pipeline-call-sSV"
    description = "A pipeline to call Somatic SV utilizing Delly"
    version = "1.0.0"
    }

params {
    // resource configuraton for entire pipeline
    max_number_of_parallel_jobs = 1
    }

methods {
    set_docker_urls = {
        params.docker_image_delly = "blcdsdockerregistry/delly:${params.delly_version}"
        params.docker_image_bcftools = "blcdsdockerregistry/bcftools:${params.bcftools_version}"
        params.docker_image_validate = "blcdsdockerregistry/validate:${params.validate_version}"
    }

    check_permissions = { path ->
        def filePath = new File(path)

        if (filePath.exists()) {
            if (filePath.canWrite()) {
                return
                }
            throw new Exception("${path} is not writable")
            }

        // Attempts to create directory if the path does not exist
        if (!filePath.mkdirs()) {
            throw new Exception("${path} does not exist and could not create")
            }
        }

    set_log_output_dir = {

        def sample

        // assumes that project and samples name are in the pipeline.config
        def reader = new FileReader(params.input_csv)
        reader.splitEachLine(',') { parts -> [sample = parts[1].split('/')[-1].split('.bam')[0]] }
        //reader.splitEachLine(',') { parts -> [sample = parts[1].tokenize('/')[-1].tokenize('.bam').join('.')] }

        def date = new Date().format('yyyyMMdd-HHmmss')
        if (params.sge_scheduler) {
              params.avere_prefix = '/data/data'
        } else {
              params.avere_prefix = '/hot/data'
        }

        if (params.blcds_registered_dataset == true) {
            if ("${params.dataset_id.length()}" != 11) {
                 throw new Exception("Dataset id must be eleven characters long")
            }
            def disease = "${params.dataset_id.substring(0,4)}"
            // Need to fill in analyte, technology, raw_od_aligned, genome, pipeline-name
            params.log_output_dir = "${params.avere_prefix}/$disease/${params.dataset_id}/${project}/${sample}/analyte/technology,raw_or_aligned/genome/logs/pipeline-name/$date"
            params.disease = "${disease}"
        } else {
            params.log_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${sample}/log-${manifest.name}-${manifest.version}-${date}"
            params.disease = null
        }

        params.sample = "${sample}"
        params.date = "${date}"
    }

    set_output_dir = {
        params.output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample}/${params.docker_image_delly.split("/")[1].replace(':', '-').toUpperCase()}"
        }

    // Process specific scope
    set_process = {
        // monitor process jobs with local (not slurm) executor
        process.executor = "local"
        // total amount of resources avaible to the pipeline
        process.maxForks = params.max_number_of_parallel_jobs
        // echo stdout of each step to stdout of pipeline
        process.echo = true
        process.cache = params.cache_intermediate_pipeline_steps
        }

    // Location of Nextflow temp directories
    set_env = {
        workDir = params.temp_dir
        }

    // Pipeline monitoring and metric files
    set_timeline = {
        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"
    }

    set_trace = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"
        }

    set_report = {
        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
        }

    set_node_config = {
        def node_cpus = SysHelper.getAvailCpus()
        def node_mem  = SysHelper.getAvailMemory().getGiga()

        if (node_cpus == 2 && node_mem >= 3 && node_mem < 5) {
            includeConfig "${projectDir}/config/F2.config"
            }
        else if (node_cpus == 32 && node_mem >= 60 && node_mem < 70) {
            includeConfig "${projectDir}/config/F32.config"
            }
        else if (node_cpus == 72 && node_mem >= 130 && node_mem < 150) {
            includeConfig "${projectDir}/config/F72.config"
            }
        else if (node_cpus == 64 && node_mem >= 900 && node_mem < 1100) {
            includeConfig "${projectDir}/config/M64.config"
            }
        else {
            throw new Exception('ERROR: System resources not as expected, unable to assign resources.')
            }
        }

    // Set up env, timeline, trace, and report above.
    setup = {
        methods.set_docker_urls()
        methods.set_log_output_dir()
        methods.set_output_dir()
        methods.check_permissions(params.log_output_dir)
        
        methods.set_env()
        methods.set_process()
        methods.set_timeline()
        methods.set_trace()
        methods.set_report()
        methods.set_node_config()
        }
    }


methods.setup()

// Enable docker
docker {
    enabled = true
    sudo = (params.sge_scheduler) ? true : false // Set to true if run on SGE

    // Pass user's UID/GID and group IDs to Docker
    uid_and_gid = "-u \$(id -u):\$(id -g)"
    all_group_ids = "\$(for i in `id --real --groups`; do echo -n \"--group-add=\$i \"; done)"

    runOptions = "${uid_and_gid} ${all_group_ids}"
    }
